{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60655d9-913e-4f28-a7cb-acf008d1dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import GEOparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    make_scorer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d50a3d-2cf4-4f41-9a91-71429837b325",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14-Jan-2026 19:50:34 DEBUG utils - Directory ../data already exists. Skipping.\n",
      "14-Jan-2026 19:50:34 INFO GEOparse - File already exist: using local version.\n",
      "14-Jan-2026 19:50:34 INFO GEOparse - Parsing ../data/GSE68951_family.soft.gz: \n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - DATABASE: GeoMiame\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SERIES: GSE68951\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - PLATFORM: GPL16770\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688368\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688369\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688370\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688371\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688372\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688373\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688374\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688375\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688376\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688377\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688378\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688379\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688380\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688381\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688382\n",
      "14-Jan-2026 19:50:34 DEBUG GEOparse - SAMPLE: GSM1688383\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688384\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688385\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688386\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688387\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688388\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688389\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688390\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688391\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688392\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688393\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688394\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688395\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688396\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688397\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688398\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688399\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688400\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688401\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688402\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688403\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688404\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688405\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688406\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688407\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688408\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688409\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688410\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688411\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688412\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688413\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688414\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688415\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688416\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688417\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688418\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688419\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688420\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688421\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688422\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688423\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688424\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688425\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688426\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688427\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688428\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688429\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688430\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688431\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688432\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688433\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688434\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688435\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688436\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688437\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688438\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688439\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688440\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688441\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688442\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688443\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688444\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688445\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688446\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688447\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688448\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688449\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688450\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688451\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688452\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688453\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688454\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688455\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688456\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688457\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688458\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688459\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688460\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688461\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688462\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688463\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688464\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688465\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688466\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688467\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688468\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688469\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688470\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688471\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688472\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688473\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688474\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688475\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688476\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688477\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688478\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688479\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688480\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688481\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688482\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688483\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688484\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688485\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688486\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688487\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688488\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688489\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688490\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688491\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688492\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688493\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688494\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688495\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688496\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688497\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688498\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688499\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688500\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688501\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688502\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688503\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688504\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688505\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688506\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688507\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688508\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688509\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688510\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688511\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688512\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688513\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688514\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688515\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688516\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688517\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688518\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688519\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688520\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688521\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688522\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688523\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688524\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688525\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688526\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688527\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688528\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688529\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688530\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688531\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688532\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688533\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688534\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688535\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688536\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688537\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688538\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688539\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688540\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688541\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688542\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688543\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688544\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688545\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688546\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688547\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688548\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688549\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688550\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688551\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688552\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688553\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688554\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688555\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688556\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688557\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688558\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688559\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688560\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688561\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688562\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688563\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688564\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688565\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688566\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688567\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688568\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688569\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688570\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688571\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688572\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688573\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688574\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688575\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688576\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688577\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688578\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688579\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688580\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688581\n",
      "14-Jan-2026 19:50:35 DEBUG GEOparse - SAMPLE: GSM1688582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SERIES: GSE68951 - 215 SAMPLES, 1 d(s)>\n"
     ]
    }
   ],
   "source": [
    "gse = GEOparse.get_GEO(\"GSE68951\", destdir=\"../data\")\n",
    "print(gse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fbcaee1-2a33-42ee-ae33-821c8b68aa13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_REF</th>\n",
       "      <th>VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>hsa-miR-100</td>\n",
       "      <td>1.936057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>hsa-miR-101</td>\n",
       "      <td>4.114362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>hsa-miR-1289</td>\n",
       "      <td>1.735477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>hsa-miR-1288</td>\n",
       "      <td>2.192123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>hsa-miR-105</td>\n",
       "      <td>1.285812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID_REF     VALUE\n",
       "1200   hsa-miR-100  1.936057\n",
       "1201   hsa-miR-101  4.114362\n",
       "1202  hsa-miR-1289  1.735477\n",
       "1203  hsa-miR-1288  2.192123\n",
       "1204   hsa-miR-105  1.285812"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = gse.gsms[\"GSM1688406\"]\n",
    "sample.table.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51533e9d-1569-42f5-89a8-70f6ea0de859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_info(gse):\n",
    "\n",
    "    data = [] \n",
    "    for gsm_id, sample in gse.gsms.items():\n",
    "\n",
    "        meta = sample.metadata[\"characteristics_ch1\"]\n",
    "\n",
    "        patient_id, timepoint, disease = None, None, None\n",
    "        \n",
    "        for item in meta:\n",
    "            if item.startswith(\"patient id\"):\n",
    "                patient_id = item.split(\":\")[1].strip()\n",
    "            if item.startswith(\"timepoint\"):\n",
    "                timepoint = int(item.split(\":\")[1].strip())\n",
    "            if item.startswith(\"disease\"):\n",
    "                disease = item.split(\":\")[1].strip()\n",
    "        if patient_id is None or timepoint is None:\n",
    "            continue\n",
    "\n",
    "        mirnas = sample.table[\"ID_REF\"].values\n",
    "        expr   = sample.table[\"VALUE\"].values.astype(float)\n",
    "\n",
    "        data.append({\n",
    "            \"gsm\": gsm_id,\n",
    "            \"patient\": patient_id,\n",
    "            \"timepoint\": timepoint,\n",
    "            \"mirna_names\": mirnas,\n",
    "            \"expression\": expr\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a73a131-6462-4652-bfb2-e5768816272d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gsm</th>\n",
       "      <th>patient</th>\n",
       "      <th>timepoint</th>\n",
       "      <th>mirna_names</th>\n",
       "      <th>expression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSM1688368</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>[hsa-miR-507, hsa-miR-548d-5p, hsa-miR-1976, h...</td>\n",
       "      <td>[1.854979099, 2.184182338, 2.532296403, 1.5180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GSM1688369</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>[hsa-miR-507, hsa-miR-548d-5p, hsa-miR-1976, h...</td>\n",
       "      <td>[1.469763084, 2.320244044, 2.111889095, 1.6172...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GSM1688370</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>[hsa-miR-507, hsa-miR-548d-5p, hsa-miR-1976, h...</td>\n",
       "      <td>[1.44937518, 2.510284729, 2.083508284, 1.23588...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSM1688371</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>[hsa-miR-507, hsa-miR-548d-5p, hsa-miR-1976, h...</td>\n",
       "      <td>[1.777522456, 2.724678628, 3.012721615, 1.3907...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GSM1688372</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>[hsa-miR-507, hsa-miR-548d-5p, hsa-miR-1976, h...</td>\n",
       "      <td>[1.538218176, 2.030509973, 2.470374907, 1.5635...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gsm patient  timepoint  \\\n",
       "0  GSM1688368       A          1   \n",
       "1  GSM1688369       A          2   \n",
       "2  GSM1688370       A          3   \n",
       "3  GSM1688371       A          4   \n",
       "4  GSM1688372       A          5   \n",
       "\n",
       "                                         mirna_names  \\\n",
       "0  [hsa-miR-507, hsa-miR-548d-5p, hsa-miR-1976, h...   \n",
       "1  [hsa-miR-507, hsa-miR-548d-5p, hsa-miR-1976, h...   \n",
       "2  [hsa-miR-507, hsa-miR-548d-5p, hsa-miR-1976, h...   \n",
       "3  [hsa-miR-507, hsa-miR-548d-5p, hsa-miR-1976, h...   \n",
       "4  [hsa-miR-507, hsa-miR-548d-5p, hsa-miR-1976, h...   \n",
       "\n",
       "                                          expression  \n",
       "0  [1.854979099, 2.184182338, 2.532296403, 1.5180...  \n",
       "1  [1.469763084, 2.320244044, 2.111889095, 1.6172...  \n",
       "2  [1.44937518, 2.510284729, 2.083508284, 1.23588...  \n",
       "3  [1.777522456, 2.724678628, 3.012721615, 1.3907...  \n",
       "4  [1.538218176, 2.030509973, 2.470374907, 1.5635...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = extract_full_info(gse)\n",
    "df_all.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7eb549-2fe6-4768-8cf8-13f945c95cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix\n",
    "X = np.vstack(df_all[\"expression\"].values)\n",
    "\n",
    "# Labels = patient identity\n",
    "y = df_all[\"patient\"].astype(\"category\").cat.codes.values\n",
    "\n",
    "# Groups = patient (CV kontrolü için)\n",
    "groups = df_all[\"patient\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42cb7368-1c8b-4ad9-a2f2-7c47eaa4621b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient\n",
       "A              8\n",
       "B              8\n",
       "C              8\n",
       "D              8\n",
       "E              8\n",
       "F              8\n",
       "G              8\n",
       "H              7\n",
       "I              8\n",
       "J              8\n",
       "K              8\n",
       "L              8\n",
       "M              8\n",
       "N              8\n",
       "O              8\n",
       "P              8\n",
       "Q              8\n",
       "R              8\n",
       "S              7\n",
       "T              8\n",
       "U              8\n",
       "V              8\n",
       "W              8\n",
       "X              7\n",
       "Y              8\n",
       "Z              6\n",
       "ZZ_control    12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.groupby(\"patient\").size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d6f0f90-a0ae-4850-ab7d-d5bd8d7f2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_classifier = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=5000)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f651154-e2f4-4460-966f-f946921ccf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_logreg(X, y, classifier=None): #NAIVE METHOD\n",
    "\n",
    "    if classifier is None:\n",
    "        classifier = LogisticRegression(max_iter=5000)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "277ac188-daf3-42ee-96e8-c502e05e5c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.67      1.00      0.80         2\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      0.00      0.00         3\n",
      "           5       0.33      0.50      0.40         2\n",
      "           6       0.00      0.00      0.00         3\n",
      "           7       0.00      0.00      0.00         2\n",
      "           8       0.00      0.00      0.00         3\n",
      "           9       0.33      0.50      0.40         2\n",
      "          10       0.00      0.00      0.00         2\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.00      0.00      0.00         2\n",
      "          13       0.00      0.00      0.00         2\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.00      0.00      0.00         3\n",
      "          16       0.50      0.50      0.50         2\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.50      0.50      0.50         2\n",
      "          19       0.00      0.00      0.00         3\n",
      "          20       0.00      0.00      0.00         2\n",
      "          21       0.33      0.33      0.33         3\n",
      "          22       0.33      0.67      0.44         3\n",
      "          23       1.00      1.00      1.00         2\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.00      0.00      0.00         2\n",
      "          26       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.22        65\n",
      "   macro avg       0.20      0.22      0.21        65\n",
      "weighted avg       0.21      0.22      0.20        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = evaluate_model_logreg(X, y, scaler_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f3e792f-4bd5-4933-b67d-ba382634796f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B',\n",
       "       'B', 'B', 'B', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'D', 'D',\n",
       "       'D', 'D', 'D', 'D', 'D', 'D', 'E', 'E', 'E', 'E', 'E', 'E', 'E',\n",
       "       'E', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'G', 'G', 'G', 'G',\n",
       "       'G', 'G', 'G', 'G', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'I', 'I',\n",
       "       'I', 'I', 'I', 'I', 'I', 'I', 'J', 'J', 'J', 'J', 'J', 'J', 'J',\n",
       "       'J', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'L', 'L', 'L', 'L',\n",
       "       'L', 'L', 'L', 'L', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'N',\n",
       "       'N', 'N', 'N', 'N', 'N', 'N', 'N', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'Q', 'Q', 'Q',\n",
       "       'Q', 'Q', 'Q', 'Q', 'Q', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'S', 'S', 'S', 'S', 'S', 'S', 'S', 'T', 'T', 'T', 'T', 'T', 'T',\n",
       "       'T', 'T', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'V', 'V', 'V',\n",
       "       'V', 'V', 'V', 'V', 'V', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W',\n",
       "       'X', 'X', 'X', 'X', 'X', 'X', 'X', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y',\n",
       "       'Y', 'Y', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'ZZ_control', 'ZZ_control',\n",
       "       'ZZ_control', 'ZZ_control', 'ZZ_control', 'ZZ_control',\n",
       "       'ZZ_control', 'ZZ_control', 'ZZ_control', 'ZZ_control',\n",
       "       'ZZ_control', 'ZZ_control'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3520f11-4570-4037-97eb-27a1d48f57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_logreg_groupKFold(X, y, groups): #Actually does not suit our problem at hand at all cause KFold is an open-set classification\n",
    "\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "    fold = 1\n",
    "\n",
    "    clf = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(max_iter=5000)\n",
    "    )\n",
    "\n",
    "    for train_idx, test_idx in gkf.split(X, y, groups):\n",
    "\n",
    "        print(f\"\\n====== Fold {fold} ======\")\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "        print(\"Accuracy:\", acc)\n",
    "        print(\"F1 macro:\", f1)\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "        accuracies.append(acc)\n",
    "        f1s.append(f1)\n",
    "        fold += 1\n",
    "\n",
    "    print(\"\\n=== FINAL LOGREG ===\")\n",
    "    print(\"Mean Accuracy:\", np.mean(accuracies))\n",
    "    print(\"Mean F1 Macro:\", np.mean(f1s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12d34435-adf6-49fd-9375-b02bc23ea01a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Fold 1 ======\n",
      "Accuracy: 0.0\n",
      "F1 macro: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "           2       0.00      0.00      0.00       0.0\n",
      "           4       0.00      0.00      0.00       0.0\n",
      "           7       0.00      0.00      0.00       7.0\n",
      "           9       0.00      0.00      0.00       8.0\n",
      "          11       0.00      0.00      0.00       0.0\n",
      "          12       0.00      0.00      0.00       0.0\n",
      "          13       0.00      0.00      0.00       0.0\n",
      "          15       0.00      0.00      0.00       8.0\n",
      "          16       0.00      0.00      0.00       0.0\n",
      "          17       0.00      0.00      0.00       0.0\n",
      "          18       0.00      0.00      0.00       0.0\n",
      "          21       0.00      0.00      0.00       8.0\n",
      "          22       0.00      0.00      0.00       0.0\n",
      "          24       0.00      0.00      0.00       0.0\n",
      "          25       0.00      0.00      0.00       0.0\n",
      "          26       0.00      0.00      0.00      12.0\n",
      "\n",
      "    accuracy                           0.00      43.0\n",
      "   macro avg       0.00      0.00      0.00      43.0\n",
      "weighted avg       0.00      0.00      0.00      43.0\n",
      "\n",
      "\n",
      "====== Fold 2 ======\n",
      "Accuracy: 0.0\n",
      "F1 macro: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "           2       0.00      0.00      0.00       0.0\n",
      "           3       0.00      0.00      0.00       0.0\n",
      "           4       0.00      0.00      0.00       8.0\n",
      "           5       0.00      0.00      0.00       0.0\n",
      "           7       0.00      0.00      0.00       0.0\n",
      "           8       0.00      0.00      0.00       0.0\n",
      "           9       0.00      0.00      0.00       0.0\n",
      "          10       0.00      0.00      0.00       8.0\n",
      "          11       0.00      0.00      0.00       0.0\n",
      "          12       0.00      0.00      0.00       8.0\n",
      "          13       0.00      0.00      0.00       0.0\n",
      "          15       0.00      0.00      0.00       0.0\n",
      "          16       0.00      0.00      0.00       8.0\n",
      "          17       0.00      0.00      0.00       0.0\n",
      "          20       0.00      0.00      0.00       0.0\n",
      "          21       0.00      0.00      0.00       0.0\n",
      "          22       0.00      0.00      0.00       8.0\n",
      "          25       0.00      0.00      0.00       6.0\n",
      "          26       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00      46.0\n",
      "   macro avg       0.00      0.00      0.00      46.0\n",
      "weighted avg       0.00      0.00      0.00      46.0\n",
      "\n",
      "\n",
      "====== Fold 3 ======\n",
      "Accuracy: 0.0\n",
      "F1 macro: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       8.0\n",
      "           3       0.00      0.00      0.00       0.0\n",
      "           5       0.00      0.00      0.00       8.0\n",
      "           7       0.00      0.00      0.00       0.0\n",
      "           9       0.00      0.00      0.00       0.0\n",
      "          11       0.00      0.00      0.00       8.0\n",
      "          12       0.00      0.00      0.00       0.0\n",
      "          15       0.00      0.00      0.00       0.0\n",
      "          16       0.00      0.00      0.00       0.0\n",
      "          17       0.00      0.00      0.00       8.0\n",
      "          20       0.00      0.00      0.00       0.0\n",
      "          21       0.00      0.00      0.00       0.0\n",
      "          24       0.00      0.00      0.00       8.0\n",
      "          25       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00      40.0\n",
      "   macro avg       0.00      0.00      0.00      40.0\n",
      "weighted avg       0.00      0.00      0.00      40.0\n",
      "\n",
      "\n",
      "====== Fold 4 ======\n",
      "Accuracy: 0.0\n",
      "F1 macro: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       8.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "           2       0.00      0.00      0.00       8.0\n",
      "           3       0.00      0.00      0.00       0.0\n",
      "           4       0.00      0.00      0.00       0.0\n",
      "           6       0.00      0.00      0.00       8.0\n",
      "           7       0.00      0.00      0.00       0.0\n",
      "           8       0.00      0.00      0.00       0.0\n",
      "           9       0.00      0.00      0.00       0.0\n",
      "          11       0.00      0.00      0.00       0.0\n",
      "          13       0.00      0.00      0.00       8.0\n",
      "          14       0.00      0.00      0.00       0.0\n",
      "          15       0.00      0.00      0.00       0.0\n",
      "          16       0.00      0.00      0.00       0.0\n",
      "          17       0.00      0.00      0.00       0.0\n",
      "          18       0.00      0.00      0.00       0.0\n",
      "          19       0.00      0.00      0.00       8.0\n",
      "          22       0.00      0.00      0.00       0.0\n",
      "          25       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00      40.0\n",
      "   macro avg       0.00      0.00      0.00      40.0\n",
      "weighted avg       0.00      0.00      0.00      40.0\n",
      "\n",
      "\n",
      "====== Fold 5 ======\n",
      "Accuracy: 0.0\n",
      "F1 macro: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           2       0.00      0.00      0.00       0.0\n",
      "           3       0.00      0.00      0.00       8.0\n",
      "           5       0.00      0.00      0.00       0.0\n",
      "           6       0.00      0.00      0.00       0.0\n",
      "           8       0.00      0.00      0.00       8.0\n",
      "          10       0.00      0.00      0.00       0.0\n",
      "          11       0.00      0.00      0.00       0.0\n",
      "          12       0.00      0.00      0.00       0.0\n",
      "          13       0.00      0.00      0.00       0.0\n",
      "          14       0.00      0.00      0.00       8.0\n",
      "          17       0.00      0.00      0.00       0.0\n",
      "          18       0.00      0.00      0.00       7.0\n",
      "          19       0.00      0.00      0.00       0.0\n",
      "          20       0.00      0.00      0.00       8.0\n",
      "          22       0.00      0.00      0.00       0.0\n",
      "          23       0.00      0.00      0.00       7.0\n",
      "\n",
      "    accuracy                           0.00      46.0\n",
      "   macro avg       0.00      0.00      0.00      46.0\n",
      "weighted avg       0.00      0.00      0.00      46.0\n",
      "\n",
      "\n",
      "=== FINAL LOGREG ===\n",
      "Mean Accuracy: 0.0\n",
      "Mean F1 Macro: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1.8549791 , 2.18418234, 2.5322964 , ..., 1.41000193, 3.72745529,\n",
       "         1.37007761],\n",
       "        [1.46976308, 2.32024404, 2.11188909, ..., 1.4279172 , 2.45035886,\n",
       "         1.35776645],\n",
       "        [1.44937518, 2.51028473, 2.08350828, ..., 1.12366167, 2.571076  ,\n",
       "         1.06683767],\n",
       "        ...,\n",
       "        [1.55135439, 2.09630509, 1.84061179, ..., 1.46840623, 4.03715339,\n",
       "         0.933782  ],\n",
       "        [1.58819243, 2.44854081, 2.43935147, ..., 1.26001547, 3.43739097,\n",
       "         1.32423004],\n",
       "        [1.96891928, 2.30610466, 2.02487855, ..., 1.21281883, 3.98844345,\n",
       "         0.86618795]]),\n",
       " array([ 0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
       "         2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,\n",
       "         6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "        15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17,\n",
       "        17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19,\n",
       "        19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21,\n",
       "        21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
       "        23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 26,\n",
       "        26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26], dtype=int8),\n",
       " array(['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B',\n",
       "        'B', 'B', 'B', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'D', 'D',\n",
       "        'D', 'D', 'D', 'D', 'D', 'D', 'E', 'E', 'E', 'E', 'E', 'E', 'E',\n",
       "        'E', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'G', 'G', 'G', 'G',\n",
       "        'G', 'G', 'G', 'G', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'I', 'I',\n",
       "        'I', 'I', 'I', 'I', 'I', 'I', 'J', 'J', 'J', 'J', 'J', 'J', 'J',\n",
       "        'J', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'L', 'L', 'L', 'L',\n",
       "        'L', 'L', 'L', 'L', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'N',\n",
       "        'N', 'N', 'N', 'N', 'N', 'N', 'N', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "        'O', 'O', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'Q', 'Q', 'Q',\n",
       "        'Q', 'Q', 'Q', 'Q', 'Q', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'S', 'T', 'T', 'T', 'T', 'T', 'T',\n",
       "        'T', 'T', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'V', 'V', 'V',\n",
       "        'V', 'V', 'V', 'V', 'V', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W',\n",
       "        'X', 'X', 'X', 'X', 'X', 'X', 'X', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y',\n",
       "        'Y', 'Y', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'ZZ_control', 'ZZ_control',\n",
       "        'ZZ_control', 'ZZ_control', 'ZZ_control', 'ZZ_control',\n",
       "        'ZZ_control', 'ZZ_control', 'ZZ_control', 'ZZ_control',\n",
       "        'ZZ_control', 'ZZ_control'], dtype=object))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model_logreg_groupKFold(X, y, groups)\n",
    "(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a61957f-f5d6-4e93-b04b-ff0c659d196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def run_basic_svm_baseline(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Naive SVM baseline (NOT attack-consistent).\n",
    "    Purpose: sanity check that SVM can learn from the data.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=0.95)),\n",
    "        (\"svm\", SVC(\n",
    "            kernel=\"rbf\",\n",
    "            C=1.0,\n",
    "            gamma=\"scale\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    print(\"Naive SVM baseline (random split)\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c70a96a9-dfcf-4b2f-822c-7531a8abdf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive SVM baseline (random split)\n",
      "Accuracy: 0.18604651162790697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       1.00      0.50      0.67         2\n",
      "           3       0.33      0.50      0.40         2\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.50      0.50      0.50         2\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       0.33      1.00      0.50         1\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.00      0.00      0.00         2\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00         2\n",
      "          16       0.00      0.00      0.00         2\n",
      "          17       0.00      0.00      0.00         1\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.00      0.00      0.00         2\n",
      "          20       0.00      0.00      0.00         2\n",
      "          21       0.25      1.00      0.40         1\n",
      "          22       0.00      0.00      0.00         2\n",
      "          23       1.00      1.00      1.00         1\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.00      0.00      0.00         1\n",
      "          26       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.19        43\n",
      "   macro avg       0.16      0.20      0.17        43\n",
      "weighted avg       0.17      0.19      0.16        43\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;pca&#x27;, PCA(n_components=0.95)),\n",
       "                (&#x27;svm&#x27;, SVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;pca&#x27;, PCA(n_components=0.95)),\n",
       "                (&#x27;svm&#x27;, SVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.95)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=0.95)),\n",
       "                ('svm', SVC())])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_basic_svm_baseline(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bacfa027-5187-4afb-8e47-c1741071a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_svm_pca_pipeline():\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA()),\n",
    "        (\"svm\", SVC(kernel=\"rbf\"))\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da77806b-253c-406f-8adb-bb7a34f6952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(pipeline, X, y, groups):\n",
    "\n",
    "    param_grid = {\n",
    "        \"pca__n_components\": [5, 10, 15, 20],\n",
    "        \"svm__C\": [0.1, 1, 10],\n",
    "        \"svm__gamma\": [\"scale\", 0.01, 0.001]\n",
    "    }\n",
    "\n",
    "    mcc_scorer = make_scorer(matthews_corrcoef)\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=mcc_scorer,\n",
    "        cv=gkf.split(X, y, groups),\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    print(\"Best Params:\", grid.best_params_)\n",
    "    print(\"Best MCC:\", grid.best_score_)\n",
    "\n",
    "    return grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d15c0172-aa1d-49c3-9803-6bcdd2f4b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, groups):\n",
    "\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "    fold = 1\n",
    "\n",
    "    for train_idx, test_idx in gkf.split(X, y, groups):\n",
    "\n",
    "        print(f\"\\n====== Fold {fold} ======\")\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "        print(\"Accuracy:\", acc)\n",
    "        print(\"F1:\", f1)\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "        accuracies.append(acc)\n",
    "        f1s.append(f1)\n",
    "        fold += 1\n",
    "\n",
    "    print(\"\\nFINAL RESULTS\")\n",
    "    print(\"Mean Accuracy:\", np.mean(accuracies))\n",
    "    print(\"Mean F1 Macro:\", np.mean(f1s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b30ee89a-3a79-4dd8-843f-49ffb9609bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm_pca_experiment(X, y, groups):\n",
    "\n",
    "    print(\"Building model\")\n",
    "    pipeline = build_svm_pca_pipeline()\n",
    "\n",
    "    print(\"Tuning hyperparameters\")\n",
    "    best_model = tune_hyperparameters(pipeline, X, y, groups)\n",
    "\n",
    "    print(\"Evaluating final model\")\n",
    "    evaluate_model(best_model, X, y, groups)\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0663cd20-ff65-43c3-9ef2-09bd07f83e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Tuning hyperparameters\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best Params: {'pca__n_components': 5, 'svm__C': 0.1, 'svm__gamma': 'scale'}\n",
      "Best MCC: 0.0\n",
      "Evaluating final model\n",
      "\n",
      "====== Fold 1 ======\n",
      "Accuracy: 0.0\n",
      "F1: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "           2       0.00      0.00      0.00       0.0\n",
      "           4       0.00      0.00      0.00       0.0\n",
      "           7       0.00      0.00      0.00       7.0\n",
      "           8       0.00      0.00      0.00       0.0\n",
      "           9       0.00      0.00      0.00       8.0\n",
      "          10       0.00      0.00      0.00       0.0\n",
      "          12       0.00      0.00      0.00       0.0\n",
      "          13       0.00      0.00      0.00       0.0\n",
      "          14       0.00      0.00      0.00       0.0\n",
      "          15       0.00      0.00      0.00       8.0\n",
      "          17       0.00      0.00      0.00       0.0\n",
      "          19       0.00      0.00      0.00       0.0\n",
      "          21       0.00      0.00      0.00       8.0\n",
      "          22       0.00      0.00      0.00       0.0\n",
      "          26       0.00      0.00      0.00      12.0\n",
      "\n",
      "    accuracy                           0.00      43.0\n",
      "   macro avg       0.00      0.00      0.00      43.0\n",
      "weighted avg       0.00      0.00      0.00      43.0\n",
      "\n",
      "\n",
      "====== Fold 2 ======\n",
      "Accuracy: 0.0\n",
      "F1: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00       8.0\n",
      "          10       0.00      0.00      0.00       8.0\n",
      "          12       0.00      0.00      0.00       8.0\n",
      "          16       0.00      0.00      0.00       8.0\n",
      "          22       0.00      0.00      0.00       8.0\n",
      "          25       0.00      0.00      0.00       6.0\n",
      "          26       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00      46.0\n",
      "   macro avg       0.00      0.00      0.00      46.0\n",
      "weighted avg       0.00      0.00      0.00      46.0\n",
      "\n",
      "\n",
      "====== Fold 3 ======\n",
      "Accuracy: 0.0\n",
      "F1: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       8.0\n",
      "           5       0.00      0.00      0.00       8.0\n",
      "          11       0.00      0.00      0.00       8.0\n",
      "          17       0.00      0.00      0.00       8.0\n",
      "          24       0.00      0.00      0.00       8.0\n",
      "          26       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00      40.0\n",
      "   macro avg       0.00      0.00      0.00      40.0\n",
      "weighted avg       0.00      0.00      0.00      40.0\n",
      "\n",
      "\n",
      "====== Fold 4 ======\n",
      "Accuracy: 0.0\n",
      "F1: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       8.0\n",
      "           2       0.00      0.00      0.00       8.0\n",
      "           6       0.00      0.00      0.00       8.0\n",
      "          13       0.00      0.00      0.00       8.0\n",
      "          19       0.00      0.00      0.00       8.0\n",
      "          26       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00      40.0\n",
      "   macro avg       0.00      0.00      0.00      40.0\n",
      "weighted avg       0.00      0.00      0.00      40.0\n",
      "\n",
      "\n",
      "====== Fold 5 ======\n",
      "Accuracy: 0.0\n",
      "F1: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00       8.0\n",
      "           8       0.00      0.00      0.00       8.0\n",
      "          14       0.00      0.00      0.00       8.0\n",
      "          18       0.00      0.00      0.00       7.0\n",
      "          20       0.00      0.00      0.00       8.0\n",
      "          23       0.00      0.00      0.00       7.0\n",
      "          26       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00      46.0\n",
      "   macro avg       0.00      0.00      0.00      46.0\n",
      "weighted avg       0.00      0.00      0.00      46.0\n",
      "\n",
      "\n",
      "FINAL RESULTS\n",
      "Mean Accuracy: 0.0\n",
      "Mean F1 Macro: 0.0\n"
     ]
    }
   ],
   "source": [
    "best_model = run_svm_pca_experiment(X, y, groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df4e5c5f-2e62-49b9-9440-712424b1e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import GEOparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    make_scorer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6319a02-485d-4e38-a60c-b27c5a486c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df_all.groupby(\"patient\")\n",
    "first = next(iter(g))\n",
    "type(first), len(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60b7e8bc-8715-40b4-9baa-14fa7178ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reidentification_cv(df): #patient-aware LOOCV\n",
    "    for patient, df_p in df.groupby(\"patient\"):\n",
    "        for test_idx in df_p.index:\n",
    "            train_idx = df.index.difference([test_idx]) #Assume that one timepoint for each patient is an unknown profile (test data).\n",
    "            yield train_idx.to_numpy(), np.array([test_idx]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43c7b82f-e12a-4f89-ad28-3894f19aee67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.497674</td>\n",
       "      <td>0.497674</td>\n",
       "      <td>0.497674</td>\n",
       "      <td>0.497674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.606573</td>\n",
       "      <td>0.495370</td>\n",
       "      <td>0.523893</td>\n",
       "      <td>215.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.610480</td>\n",
       "      <td>0.497674</td>\n",
       "      <td>0.526764</td>\n",
       "      <td>215.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.000000  0.000000  0.000000    8.000000\n",
       "1              0.200000  0.375000  0.260870    8.000000\n",
       "2              0.400000  0.750000  0.521739    8.000000\n",
       "3              0.750000  0.375000  0.500000    8.000000\n",
       "4              0.400000  0.250000  0.307692    8.000000\n",
       "5              0.666667  0.750000  0.705882    8.000000\n",
       "6              0.428571  0.375000  0.400000    8.000000\n",
       "7              0.600000  0.428571  0.500000    7.000000\n",
       "8              1.000000  0.375000  0.545455    8.000000\n",
       "9              0.714286  0.625000  0.666667    8.000000\n",
       "10             0.500000  0.125000  0.200000    8.000000\n",
       "11             0.300000  0.375000  0.333333    8.000000\n",
       "12             1.000000  0.375000  0.545455    8.000000\n",
       "13             0.400000  0.500000  0.444444    8.000000\n",
       "14             1.000000  0.750000  0.857143    8.000000\n",
       "15             0.428571  0.375000  0.400000    8.000000\n",
       "16             0.666667  0.500000  0.571429    8.000000\n",
       "17             0.384615  0.625000  0.476190    8.000000\n",
       "18             0.833333  0.714286  0.769231    7.000000\n",
       "19             0.857143  0.750000  0.800000    8.000000\n",
       "20             0.800000  0.500000  0.615385    8.000000\n",
       "21             0.714286  0.625000  0.666667    8.000000\n",
       "22             0.666667  0.750000  0.705882    8.000000\n",
       "23             1.000000  0.857143  0.923077    7.000000\n",
       "24             1.000000  0.750000  0.857143    8.000000\n",
       "25             0.000000  0.000000  0.000000    6.000000\n",
       "26             0.666667  0.500000  0.571429   12.000000\n",
       "accuracy       0.497674  0.497674  0.497674    0.497674\n",
       "macro avg      0.606573  0.495370  0.523893  215.000000\n",
       "weighted avg   0.610480  0.497674  0.526764  215.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=0.9)),  # %90 variance\n",
    "    (\"logreg\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(reidentification_cv(df_all), 1):\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "report = classification_report(\n",
    "    y_true_all,\n",
    "    y_pred_all,\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8be72bb-9c16-420a-b407-b5dbd469be44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.409302</td>\n",
       "      <td>0.409302</td>\n",
       "      <td>0.409302</td>\n",
       "      <td>0.409302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.468396</td>\n",
       "      <td>0.405864</td>\n",
       "      <td>0.400535</td>\n",
       "      <td>215.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.472073</td>\n",
       "      <td>0.409302</td>\n",
       "      <td>0.404468</td>\n",
       "      <td>215.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.050000  0.125000  0.071429    8.000000\n",
       "1              0.235294  0.500000  0.320000    8.000000\n",
       "2              0.384615  0.625000  0.476190    8.000000\n",
       "3              0.600000  0.375000  0.461538    8.000000\n",
       "4              1.000000  0.125000  0.222222    8.000000\n",
       "5              0.666667  0.750000  0.705882    8.000000\n",
       "6              0.000000  0.000000  0.000000    8.000000\n",
       "7              0.400000  0.285714  0.333333    7.000000\n",
       "8              0.000000  0.000000  0.000000    8.000000\n",
       "9              0.428571  0.375000  0.400000    8.000000\n",
       "10             0.222222  0.250000  0.235294    8.000000\n",
       "11             0.312500  0.625000  0.416667    8.000000\n",
       "12             0.500000  0.125000  0.200000    8.000000\n",
       "13             0.666667  0.500000  0.571429    8.000000\n",
       "14             1.000000  0.625000  0.769231    8.000000\n",
       "15             0.222222  0.250000  0.235294    8.000000\n",
       "16             0.250000  0.125000  0.166667    8.000000\n",
       "17             0.285714  0.750000  0.413793    8.000000\n",
       "18             0.833333  0.714286  0.769231    7.000000\n",
       "19             0.333333  0.250000  0.285714    8.000000\n",
       "20             0.200000  0.125000  0.153846    8.000000\n",
       "21             0.666667  0.750000  0.705882    8.000000\n",
       "22             0.500000  0.625000  0.555556    8.000000\n",
       "23             1.000000  1.000000  1.000000    7.000000\n",
       "24             0.500000  0.250000  0.333333    8.000000\n",
       "25             0.500000  0.166667  0.250000    6.000000\n",
       "26             0.888889  0.666667  0.761905   12.000000\n",
       "accuracy       0.409302  0.409302  0.409302    0.409302\n",
       "macro avg      0.468396  0.405864  0.400535  215.000000\n",
       "weighted avg   0.472073  0.409302  0.404468  215.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf_svm = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=0.9)),\n",
    "    (\"svm\", SVC(\n",
    "        kernel=\"rbf\",\n",
    "        C=10,\n",
    "        gamma=\"scale\",\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "y_true_all_svm = []\n",
    "y_pred_all_svm = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(reidentification_cv(df_all), 1):\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    clf_svm.fit(X_train, y_train)\n",
    "    y_pred = clf_svm.predict(X_test)\n",
    "\n",
    "    y_true_all_svm.extend(y_test)\n",
    "    y_pred_all_svm.extend(y_pred)\n",
    "\n",
    "report_svm = classification_report(\n",
    "    y_true_all_svm,\n",
    "    y_pred_all_svm,\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "report_svm_df = pd.DataFrame(report_svm).transpose()\n",
    "report_svm_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3505cd25-93ec-4935-ae2b-836956268c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.144186</td>\n",
       "      <td>0.144186</td>\n",
       "      <td>0.144186</td>\n",
       "      <td>0.144186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.253631</td>\n",
       "      <td>0.132937</td>\n",
       "      <td>0.145540</td>\n",
       "      <td>215.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.251282</td>\n",
       "      <td>0.144186</td>\n",
       "      <td>0.149839</td>\n",
       "      <td>215.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.000000  0.000000  0.000000    8.000000\n",
       "1              0.222222  0.250000  0.235294    8.000000\n",
       "2              0.125000  0.250000  0.166667    8.000000\n",
       "3              0.000000  0.000000  0.000000    8.000000\n",
       "4              0.000000  0.000000  0.000000    8.000000\n",
       "5              0.250000  0.250000  0.250000    8.000000\n",
       "6              0.000000  0.000000  0.000000    8.000000\n",
       "7              0.000000  0.000000  0.000000    7.000000\n",
       "8              0.047619  0.125000  0.068966    8.000000\n",
       "9              1.000000  0.250000  0.400000    8.000000\n",
       "10             0.000000  0.000000  0.000000    8.000000\n",
       "11             0.333333  0.125000  0.181818    8.000000\n",
       "12             1.000000  0.125000  0.222222    8.000000\n",
       "13             0.000000  0.000000  0.000000    8.000000\n",
       "14             1.000000  0.375000  0.545455    8.000000\n",
       "15             0.250000  0.125000  0.166667    8.000000\n",
       "16             0.000000  0.000000  0.000000    8.000000\n",
       "17             0.142857  0.125000  0.133333    8.000000\n",
       "18             1.000000  0.142857  0.250000    7.000000\n",
       "19             0.000000  0.000000  0.000000    8.000000\n",
       "20             0.000000  0.000000  0.000000    8.000000\n",
       "21             0.000000  0.000000  0.000000    8.000000\n",
       "22             0.166667  0.125000  0.142857    8.000000\n",
       "23             1.000000  0.571429  0.727273    7.000000\n",
       "24             0.000000  0.000000  0.000000    8.000000\n",
       "25             0.000000  0.000000  0.000000    6.000000\n",
       "26             0.310345  0.750000  0.439024   12.000000\n",
       "accuracy       0.144186  0.144186  0.144186    0.144186\n",
       "macro avg      0.253631  0.132937  0.145540  215.000000\n",
       "weighted avg   0.251282  0.144186  0.149839  215.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "clf_nb = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"gnb\", GaussianNB())\n",
    "])\n",
    "\n",
    "y_true_all_nb = []\n",
    "y_pred_all_nb = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(reidentification_cv(df_all), 1):\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    clf_nb.fit(X_train, y_train)\n",
    "    y_pred = clf_nb.predict(X_test)\n",
    "\n",
    "    y_true_all_nb.extend(y_test)\n",
    "    y_pred_all_nb.extend(y_pred)\n",
    "\n",
    "report_nb = classification_report(\n",
    "    y_true_all_nb,\n",
    "    y_pred_all_nb,\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "report_nb_df = pd.DataFrame(report_nb).transpose()\n",
    "report_nb_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bab04b0c-79ab-4f4e-95fd-57da093b890e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogReg_precision</th>\n",
       "      <th>LogReg_recall</th>\n",
       "      <th>LogReg_f1</th>\n",
       "      <th>SVM_precision</th>\n",
       "      <th>SVM_recall</th>\n",
       "      <th>SVM_f1</th>\n",
       "      <th>Bayes_precision</th>\n",
       "      <th>Bayes_recall</th>\n",
       "      <th>Bayes_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.606573</td>\n",
       "      <td>0.49537</td>\n",
       "      <td>0.523893</td>\n",
       "      <td>0.468396</td>\n",
       "      <td>0.405864</td>\n",
       "      <td>0.400535</td>\n",
       "      <td>0.253631</td>\n",
       "      <td>0.132937</td>\n",
       "      <td>0.14554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           LogReg_precision  LogReg_recall  LogReg_f1  SVM_precision  \\\n",
       "macro avg          0.606573        0.49537   0.523893       0.468396   \n",
       "\n",
       "           SVM_recall    SVM_f1  Bayes_precision  Bayes_recall  Bayes_f1  \n",
       "macro avg    0.405864  0.400535         0.253631      0.132937   0.14554  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    \"LogReg_precision\": report_df.loc[\"macro avg\", \"precision\"],\n",
    "    \"LogReg_recall\": report_df.loc[\"macro avg\", \"recall\"],\n",
    "    \"LogReg_f1\": report_df.loc[\"macro avg\", \"f1-score\"],\n",
    "    \"SVM_precision\": report_svm_df.loc[\"macro avg\", \"precision\"],\n",
    "    \"SVM_recall\": report_svm_df.loc[\"macro avg\", \"recall\"],\n",
    "    \"SVM_f1\": report_svm_df.loc[\"macro avg\", \"f1-score\"],\n",
    "    \"Bayes_precision\": report_nb_df.loc[\"macro avg\", \"precision\"],\n",
    "    \"Bayes_recall\": report_nb_df.loc[\"macro avg\", \"recall\"],\n",
    "    \"Bayes_f1\": report_nb_df.loc[\"macro avg\", \"f1-score\"],\n",
    "\n",
    "}, index=[\"macro avg\"])\n",
    "\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbe78a25-6a41-4009-a90f-99242bf7b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def naive_bayes_reidentification_cv(X, y, cv_iterator, top_k=5):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_topk = []\n",
    "\n",
    "    for train_idx, test_idx in cv_iterator:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"gnb\", GaussianNB())\n",
    "        ])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Top-1\n",
    "        y_hat = model.predict(X_test)[0]\n",
    "        y_true.append(y_test[0])\n",
    "        y_pred.append(y_hat)\n",
    "\n",
    "        # Top-k (re-identification relevant)\n",
    "        proba = model.predict_proba(X_test)[0]\n",
    "        classes = model.named_steps[\"gnb\"].classes_\n",
    "        topk = classes[np.argsort(proba)[::-1][:top_k]]\n",
    "        y_topk.append(y_test[0] in topk)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    topk_acc = np.mean(y_topk)\n",
    "\n",
    "    return acc, topk_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01a1e482-2b9b-4e80-b039-7cd1bbdebc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.144\n",
      "Naive Bayes Top-5 Accuracy: 0.353\n"
     ]
    }
   ],
   "source": [
    "acc, top5 = naive_bayes_reidentification_cv(\n",
    "    X,\n",
    "    y,\n",
    "    reidentification_cv(df_all),\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"Naive Bayes Accuracy: {acc:.3f}\")\n",
    "print(f\"Naive Bayes Top-5 Accuracy: {top5:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37400ef9-92f0-47ab-b186-0f69c12f6b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positive_pairs(X, y):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    for patient in np.unique(y):\n",
    "        idx = np.where(y == patient)[0]\n",
    "        for i in range(len(idx)):\n",
    "            for j in range(i + 1, len(idx)):\n",
    "                pairs.append([X[idx[i]], X[idx[j]]])\n",
    "                labels.append(1)\n",
    "\n",
    "    return np.array(pairs), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f37227df-b855-4455-a9c8-af37b4ae9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_negative_pairs_random(X, y, n_pairs):\n",
    "    pairs, labels = [], []\n",
    "    patients = np.unique(y)\n",
    "\n",
    "    for _ in range(n_pairs):\n",
    "        p1, p2 = np.random.choice(patients, 2, replace=False)\n",
    "        i = np.random.choice(np.where(y == p1)[0])\n",
    "        j = np.random.choice(np.where(y == p2)[0])\n",
    "        pairs.append([X[i], X[j]])\n",
    "        labels.append(0)\n",
    "\n",
    "    return np.array(pairs), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddf0ddcd-363d-444d-9ff4-294920f114e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp, yp = make_positive_pairs(X_train, y_train)\n",
    "Xn, yn = make_negative_pairs_random(X_train, y_train, len(yp))\n",
    "\n",
    "X_pairs = np.vstack([Xp, Xn]) #forms a 3D tensor\n",
    "y_pairs = np.hstack([yp, yn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c938d28-af0e-4112-8858-d68b661023f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def build_base_network(input_dim):\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation=\"relu\")(inp)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    return Model(inp, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2045afae-9e72-4ea7-864a-7afd43b4364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "base = build_base_network(input_dim)\n",
    "\n",
    "input_a = layers.Input(shape=(input_dim,))\n",
    "input_b = layers.Input(shape=(input_dim,))\n",
    "\n",
    "emb_a = base(input_a)\n",
    "emb_b = base(input_b) ## Weight sharing neden şart?\n",
    "\n",
    "distance = layers.Lambda(\n",
    "    lambda x: tf.reduce_sum(tf.square(x[0] - x[1]), axis=1, keepdims=True)\n",
    ")([emb_a, emb_b]) \n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(distance) \n",
    "\n",
    "siamese = Model([input_a, input_b], output)\n",
    "siamese.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b9ef68f-bb04-4a08-ab34-34e869af9e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x131ded360>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese.fit(\n",
    "    [X_pairs[:,0], X_pairs[:,1]],\n",
    "    y_pairs,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "832113ca-9374-4d94-9e45-520743d06aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_patient_siamese(siamese, x_test, X_train, y_train):\n",
    "    scores = []\n",
    "\n",
    "    for x_tr, patient in zip(X_train, y_train):\n",
    "        prob = siamese.predict(\n",
    "            [x_test.reshape(1,-1), x_tr.reshape(1,-1)],\n",
    "            verbose=0\n",
    "        )[0][0]\n",
    "        scores.append((patient, prob))\n",
    "\n",
    "    df_scores = pd.DataFrame(scores, columns=[\"patient\", \"score\"])\n",
    "    return df_scores.groupby(\"patient\")[\"score\"].mean().idxmax() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2261858d-f787-4198-9ac4-c39dfaa33422",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for train_idx, test_idx in reidentification_cv(df_all):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # pair üret, train et (yukarıdaki adımlar)\n",
    "    # sonra predict_patient_siamese\n",
    "\n",
    "    y_pred = predict_patient_siamese(\n",
    "        siamese, X_test[0], X_train, y_train\n",
    "    )\n",
    "\n",
    "    y_true_all.append(y_test[0])\n",
    "    y_pred_all.append(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fe94847-0cee-4ba9-928d-e56630083f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING:\n",
    "# At this stage, the model is NOT reset between folds.\n",
    "# This introduces data leakage and leads to overly optimistic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a95c190d-117a-4bab-8b64-f2459c40da82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.051643</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.097778</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.051163</td>\n",
       "      <td>0.051163</td>\n",
       "      <td>0.051163</td>\n",
       "      <td>0.051163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.033951</td>\n",
       "      <td>0.003621</td>\n",
       "      <td>215.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.051163</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>215.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.000000  0.000000  0.000000    8.000000\n",
       "1              0.000000  0.000000  0.000000    8.000000\n",
       "2              0.000000  0.000000  0.000000    8.000000\n",
       "3              0.000000  0.000000  0.000000    8.000000\n",
       "4              0.000000  0.000000  0.000000    8.000000\n",
       "5              0.000000  0.000000  0.000000    8.000000\n",
       "6              0.000000  0.000000  0.000000    8.000000\n",
       "7              0.000000  0.000000  0.000000    7.000000\n",
       "8              0.000000  0.000000  0.000000    8.000000\n",
       "9              0.000000  0.000000  0.000000    8.000000\n",
       "10             0.000000  0.000000  0.000000    8.000000\n",
       "11             0.000000  0.000000  0.000000    8.000000\n",
       "12             0.000000  0.000000  0.000000    8.000000\n",
       "13             0.000000  0.000000  0.000000    8.000000\n",
       "14             0.000000  0.000000  0.000000    8.000000\n",
       "15             0.000000  0.000000  0.000000    8.000000\n",
       "16             0.000000  0.000000  0.000000    8.000000\n",
       "17             0.000000  0.000000  0.000000    8.000000\n",
       "18             0.000000  0.000000  0.000000    7.000000\n",
       "19             0.000000  0.000000  0.000000    8.000000\n",
       "20             0.000000  0.000000  0.000000    8.000000\n",
       "21             0.000000  0.000000  0.000000    8.000000\n",
       "22             0.000000  0.000000  0.000000    8.000000\n",
       "23             0.000000  0.000000  0.000000    7.000000\n",
       "24             0.000000  0.000000  0.000000    8.000000\n",
       "25             0.000000  0.000000  0.000000    6.000000\n",
       "26             0.051643  0.916667  0.097778   12.000000\n",
       "accuracy       0.051163  0.051163  0.051163    0.051163\n",
       "macro avg      0.001913  0.033951  0.003621  215.000000\n",
       "weighted avg   0.002882  0.051163  0.005457  215.000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(reidentification_cv(df_all), 1):\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    Xp, yp = make_positive_pairs(X_train, y_train)\n",
    "    Xn, yn = make_negative_pairs_random(X_train, y_train, n_pairs=len(yp))\n",
    "\n",
    "    X_pairs = np.vstack([Xp, Xn])\n",
    "    y_pairs = np.hstack([yp, yn])\n",
    "\n",
    "    siamese.fit(\n",
    "        [X_pairs[:, 0], X_pairs[:, 1]],\n",
    "        y_pairs,\n",
    "        batch_size=16,\n",
    "        epochs=10,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    y_pred = predict_patient_siamese(\n",
    "        siamese,\n",
    "        X_test[0],\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    "\n",
    "    y_true_all.append(y_test[0])\n",
    "    y_pred_all.append(y_pred)\n",
    "\n",
    "report_siamese = classification_report(\n",
    "    y_true_all,\n",
    "    y_pred_all,\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "report_siamese_df = pd.DataFrame(report_siamese).transpose()\n",
    "report_siamese_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f26cc894-2edc-49e7-a3ea-ccfd30ba4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_negative_pairs_hard(X, y, n_pairs, n_components=50):\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    D = euclidean_distances(X_pca, X_pca)\n",
    "    mask = y[:, None] != y[None, :]\n",
    "    D = np.where(mask, D, np.inf)\n",
    "\n",
    "    pairs, labels = [], []\n",
    "\n",
    "    flat_idx = np.argsort(D, axis=None)[:n_pairs]\n",
    "\n",
    "    for idx in flat_idx:\n",
    "        i, j = np.unravel_index(idx, D.shape)\n",
    "        pairs.append([X[i], X[j]])\n",
    "        labels.append(0)\n",
    "\n",
    "    return np.array(pairs), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83feef13-0cef-42bc-a13d-40a772839feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def build_siamese(input_dim):\n",
    "\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(\n",
    "        256, activation=\"relu\",\n",
    "        kernel_regularizer=regularizers.l2(1e-4)\n",
    "    )(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = layers.Dense(\n",
    "        128, activation=\"relu\",\n",
    "        kernel_regularizer=regularizers.l2(1e-4)\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    embedding = layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=1))(x)\n",
    "\n",
    "    base = Model(inp, embedding)\n",
    "\n",
    "    a = layers.Input(shape=(input_dim,))\n",
    "    b = layers.Input(shape=(input_dim,))\n",
    "\n",
    "    ea = base(a)\n",
    "    eb = base(b)\n",
    "\n",
    "    dist = layers.Lambda(\n",
    "        lambda x: tf.reduce_sum(tf.square(x[0] - x[1]), axis=1, keepdims=True)\n",
    "    )([ea, eb])\n",
    "\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(dist)\n",
    "\n",
    "    model = Model([a, b], out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"binary_crossentropy\"\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "569ed572-d6ee-4811-be66-567e2129e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_patient_siamese(model, x_test, X_train, y_train):\n",
    "\n",
    "    scores = []\n",
    "    for x_tr, p in zip(X_train, y_train):\n",
    "        s = model.predict(\n",
    "            [x_test.reshape(1,-1), x_tr.reshape(1,-1)],\n",
    "            verbose=0\n",
    "        )[0][0]\n",
    "        scores.append((p, s))\n",
    "\n",
    "    df = pd.DataFrame(scores, columns=[\"patient\", \"score\"])\n",
    "    return df.groupby(\"patient\")[\"score\"].mean().idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "682f25ce-f640-4e1f-89e6-8ceb771b869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, regularizers\n",
    "## BU BI YERE GELICEK AMA NEREYE AMK Pair verification’da model iki örneğin benzerliğini öğrenir; re-identification’da ise bu öğrenilmiş embedding uzayı kullanılarak tek bir örnek, en yakın kimliğe atanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f43a4a1-881a-4751-931d-55ca4bac85d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.07      0.12      0.09         8\n",
      "           2       0.04      0.12      0.06         8\n",
      "           3       0.50      0.12      0.20         8\n",
      "           4       0.00      0.00      0.00         8\n",
      "           5       0.11      0.12      0.12         8\n",
      "           6       0.50      0.25      0.33         8\n",
      "           7       0.00      0.00      0.00         7\n",
      "           8       0.12      0.25      0.16         8\n",
      "           9       0.15      0.50      0.23         8\n",
      "          10       0.00      0.00      0.00         8\n",
      "          11       0.08      0.12      0.10         8\n",
      "          12       0.00      0.00      0.00         8\n",
      "          13       0.14      0.12      0.13         8\n",
      "          14       0.40      0.50      0.44         8\n",
      "          15       0.33      0.12      0.18         8\n",
      "          16       0.20      0.12      0.15         8\n",
      "          17       0.17      0.12      0.14         8\n",
      "          18       0.29      0.29      0.29         7\n",
      "          19       0.00      0.00      0.00         8\n",
      "          20       0.00      0.00      0.00         8\n",
      "          21       0.00      0.00      0.00         8\n",
      "          22       0.50      0.50      0.50         8\n",
      "          23       0.07      0.29      0.12         7\n",
      "          24       0.00      0.00      0.00         8\n",
      "          25       0.00      0.00      0.00         6\n",
      "          26       0.23      0.25      0.24        12\n",
      "\n",
      "    accuracy                           0.15       215\n",
      "   macro avg       0.14      0.15      0.13       215\n",
      "weighted avg       0.15      0.15      0.13       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = [], []\n",
    "\n",
    "for train_idx, test_idx in reidentification_cv(df_all):\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    Xp, yp = make_positive_pairs(X_train, y_train)\n",
    "    Xn, yn = make_negative_pairs_random(X_train, y_train, len(yp))\n",
    "\n",
    "    X_pairs = np.vstack([Xp, Xn])\n",
    "    y_pairs = np.hstack([yp, yn])\n",
    "\n",
    "    model = build_siamese(X.shape[1])\n",
    "    model.fit(\n",
    "        [X_pairs[:,0], X_pairs[:,1]],\n",
    "        y_pairs,\n",
    "        epochs=3,\n",
    "        batch_size=16,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    pred = predict_patient_siamese(model, X_test[0], X_train, y_train)\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(pred)\n",
    "\n",
    "print(classification_report(y_true, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8ee6df5-b35b-435d-8798-30b1ee12e841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.20      0.12      0.15         8\n",
      "           2       0.06      0.25      0.10         8\n",
      "           3       0.33      0.12      0.18         8\n",
      "           4       0.00      0.00      0.00         8\n",
      "           5       0.22      0.62      0.32         8\n",
      "           6       0.75      0.38      0.50         8\n",
      "           7       0.00      0.00      0.00         7\n",
      "           8       0.07      0.25      0.11         8\n",
      "           9       0.33      0.38      0.35         8\n",
      "          10       0.00      0.00      0.00         8\n",
      "          11       0.00      0.00      0.00         8\n",
      "          12       0.00      0.00      0.00         8\n",
      "          13       0.17      0.12      0.14         8\n",
      "          14       0.31      0.50      0.38         8\n",
      "          15       0.00      0.00      0.00         8\n",
      "          16       0.00      0.00      0.00         8\n",
      "          17       0.00      0.00      0.00         8\n",
      "          18       0.00      0.00      0.00         7\n",
      "          19       0.14      0.12      0.13         8\n",
      "          20       0.00      0.00      0.00         8\n",
      "          21       1.00      0.12      0.22         8\n",
      "          22       0.00      0.00      0.00         8\n",
      "          23       0.17      0.43      0.24         7\n",
      "          24       0.38      0.38      0.38         8\n",
      "          25       0.00      0.00      0.00         6\n",
      "          26       0.33      0.08      0.13        12\n",
      "\n",
      "    accuracy                           0.14       215\n",
      "   macro avg       0.17      0.14      0.12       215\n",
      "weighted avg       0.17      0.14      0.13       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for train_idx, test_idx in reidentification_cv(df_all):\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    Xp, yp = make_positive_pairs(X_train, y_train)\n",
    "    Xn, yn = make_negative_pairs_hard(X_train, y_train, len(yp))\n",
    "\n",
    "    X_pairs = np.vstack([Xp, Xn])\n",
    "    y_pairs = np.hstack([yp, yn])\n",
    "\n",
    "    model = build_siamese(X.shape[1])\n",
    "    model.fit(\n",
    "        [X_pairs[:,0], X_pairs[:,1]],\n",
    "        y_pairs,\n",
    "        epochs=3,\n",
    "        batch_size=16,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    pred = predict_patient_siamese(model, X_test[0], X_train, y_train)\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(pred)\n",
    "\n",
    "print(classification_report(y_true, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e62011d5-e8bb-421a-a27f-fe0444468ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noctrl = df_all[df_all[\"patient\"] != \"ZZ_control\"].copy()\n",
    "X_nc = np.vstack(df_noctrl[\"expression\"].values)\n",
    "y_nc = df_noctrl[\"patient\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63857c00-fa55-4c97-9bf9-272d696f8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e451c-8f6d-49c3-b02f-80dbee47bef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = [], []\n",
    "\n",
    "for train_idx, test_idx in reidentification_cv(df_all):\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    Xp, yp = make_positive_pairs(X_train, y_train)\n",
    "    Xn, yn = make_negative_pairs_hard(X_train, y_train, len(yp))\n",
    "\n",
    "    X_pairs = np.vstack([Xp, Xn])\n",
    "    y_pairs = np.hstack([yp, yn])\n",
    "\n",
    "    model = build_siamese(X.shape[1])\n",
    "\n",
    "    model.fit(\n",
    "        [X_pairs[:,0], X_pairs[:,1]],\n",
    "        y_pairs,\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"loss\",\n",
    "                patience=3,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pred = predict_patient_siamese(\n",
    "        model, X_test[0], X_train, y_train\n",
    "    )\n",
    "\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(pred)\n",
    "\n",
    "print(classification_report(y_true, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5af3b1-32e8-4b34-bffc-c4ffeafd2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp, yp = make_positive_pairs(X, y)\n",
    "Xn, yn = make_negative_pairs_hard(X, y, n_pairs=3*len(yp))\n",
    "\n",
    "X_pairs = np.vstack([Xp, Xn])\n",
    "y_pairs = np.hstack([yp, yn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b91c91-fca5-40fa-b664-5d0fef6815d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_siamese(X.shape[1])\n",
    "\n",
    "model.fit(\n",
    "    [X_pairs[:,0], X_pairs[:,1]],\n",
    "    y_pairs,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"loss\",\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345ba07-4a09-4a17-bcfe-595cb866a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = model.layers[2]  # base network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0067dcb-03b2-445c-a8b3-072dacbd1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_patient_embedding(x_test, X_train, y_train, embed):\n",
    "\n",
    "    z_test = embed.predict(x_test.reshape(1,-1), verbose=0)\n",
    "    Z_train = embed.predict(X_train, verbose=0)\n",
    "\n",
    "    dists = euclidean_distances(z_test, Z_train)[0]\n",
    "    df = pd.DataFrame({\n",
    "        \"patient\": y_train,\n",
    "        \"dist\": dists\n",
    "    })\n",
    "\n",
    "    return df.groupby(\"patient\")[\"dist\"].mean().idxmin()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3db9f0-9de7-4866-8c7f-2c1941e1bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = [], []\n",
    "\n",
    "for train_idx, test_idx in reidentification_cv(df_all):\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    pred = predict_patient_embedding(\n",
    "        X_test[0], X_train, y_train, embedding_model\n",
    "    )\n",
    "\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(pred)\n",
    "\n",
    "print(classification_report(y_true, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86daa4cc-43ef-4f81-9d1f-d036e3f0b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "\n",
    "# --- reidentification CV (seninki) ---\n",
    "def reidentification_cv(df):\n",
    "    for patient, df_p in df.groupby(\"patient\"):\n",
    "        for test_idx in df_p.index:\n",
    "            train_idx = df.index.difference([test_idx])\n",
    "            yield train_idx.to_numpy(), np.array([test_idx])\n",
    "\n",
    "# --- pair makers (seninki) ---\n",
    "def make_positive_pairs(X, y):\n",
    "    pairs, labels = [], []\n",
    "    for p in np.unique(y):\n",
    "        idx = np.where(y == p)[0]\n",
    "        for i in range(len(idx)):\n",
    "            for j in range(i + 1, len(idx)):\n",
    "                pairs.append([X[idx[i]], X[idx[j]]])\n",
    "                labels.append(1)\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "def make_negative_pairs_hard(X, y, n_pairs, n_components=50):\n",
    "    pca = PCA(n_components=min(n_components, X.shape[1]))\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    D = euclidean_distances(X_pca, X_pca)\n",
    "    mask = y[:, None] != y[None, :]\n",
    "    D = np.where(mask, D, np.inf)\n",
    "\n",
    "    flat_idx = np.argsort(D, axis=None)[:n_pairs]\n",
    "    pairs, labels = [], []\n",
    "    for idx in flat_idx:\n",
    "        i, j = np.unravel_index(idx, D.shape)\n",
    "        pairs.append([X[i], X[j]])\n",
    "        labels.append(0)\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "# --- siamese builder (seninki ama base'i ayrıca döndürüyorum) ---\n",
    "def build_siamese(input_dim):\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(256, activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(1e-4))(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = layers.Dense(128, activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    embedding = layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=1))(x)\n",
    "\n",
    "    base = Model(inp, embedding, name=\"embedder\")\n",
    "\n",
    "    a = layers.Input(shape=(input_dim,))\n",
    "    b = layers.Input(shape=(input_dim,))\n",
    "    ea = base(a)\n",
    "    eb = base(b)\n",
    "\n",
    "    dist = layers.Lambda(lambda z: tf.reduce_sum(tf.square(z[0]-z[1]),\n",
    "                                                 axis=1, keepdims=True))([ea, eb])\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(dist)\n",
    "\n",
    "    model = Model([a, b], out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss=\"binary_crossentropy\")\n",
    "    return model, base\n",
    "\n",
    "# --- embedding ile patient tahmini (mean distance) ---\n",
    "def predict_patient_embedding(embedder, x_test, X_train, y_train):\n",
    "    z_test = embedder.predict(x_test.reshape(1,-1), verbose=0)\n",
    "    Z_train = embedder.predict(X_train, verbose=0)\n",
    "    d = euclidean_distances(z_test, Z_train)[0]\n",
    "    df = pd.DataFrame({\"patient\": y_train, \"dist\": d})\n",
    "    return df.groupby(\"patient\")[\"dist\"].mean().idxmin()\n",
    "\n",
    "# =========================\n",
    "#   LEAKAGE-FREE EVAL\n",
    "# =========================\n",
    "\n",
    "# X ve y burada hazır olmalı:\n",
    "# X = np.vstack(df_all[\"expression\"].values)\n",
    "# y = df_all[\"patient\"].values  # string label önerilir\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(reidentification_cv(df_all), 1):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # pairs sadece TRAIN'den\n",
    "    Xp, yp = make_positive_pairs(X_train, y_train)\n",
    "    Xn, yn = make_negative_pairs_hard(X_train, y_train, n_pairs=len(yp))\n",
    "\n",
    "    X_pairs = np.vstack([Xp, Xn])\n",
    "    y_pairs = np.hstack([yp, yn])\n",
    "\n",
    "    # her fold'da yeni model (doğru ama maliyetli)\n",
    "    siamese, embedder = build_siamese(X_train.shape[1])\n",
    "\n",
    "    siamese.fit(\n",
    "        [X_pairs[:,0], X_pairs[:,1]],\n",
    "        y_pairs,\n",
    "        epochs=30,\n",
    "        batch_size=64,\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"loss\", patience=5, restore_best_weights=True\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    pred = predict_patient_embedding(embedder, X_test[0], X_train, y_train)\n",
    "\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(pred)\n",
    "\n",
    "print(classification_report(y_true, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96978e84-d1b3-4076-aa5c-02ac3b4f80d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "\n",
    "# ============================================\n",
    "#  1. CV ve Pair Fonksiyonları\n",
    "# ============================================\n",
    "\n",
    "def reidentification_cv(df):\n",
    "    \"\"\"Her hasta için her örneği bir kez test olarak bırak\"\"\"\n",
    "    for patient, df_p in df.groupby(\"patient\"):\n",
    "        for test_idx in df_p.index:\n",
    "            train_idx = df.index.difference([test_idx])\n",
    "            yield train_idx.to_numpy(), np.array([test_idx])\n",
    "\n",
    "def make_positive_pairs(X, y):\n",
    "    \"\"\"Aynı hastadan gelen tüm çiftleri oluştur\"\"\"\n",
    "    pairs, labels = [], []\n",
    "    for p in np.unique(y):\n",
    "        idx = np.where(y == p)[0]\n",
    "        for i in range(len(idx)):\n",
    "            for j in range(i + 1, len(idx)):\n",
    "                pairs.append([X[idx[i]], X[idx[j]]])\n",
    "                labels.append(1)\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "def make_negative_pairs_hard(X, y, n_pairs, n_components=100):\n",
    "    \"\"\"En yakın (hard) negative çiftleri bul\"\"\"\n",
    "    # PCA ile boyut azalt\n",
    "    pca = PCA(n_components=min(n_components, X.shape[1]))\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Mesafe matrisi hesapla\n",
    "    D = euclidean_distances(X_pca, X_pca)\n",
    "    \n",
    "    # Sadece farklı hastaları maskele\n",
    "    mask = y[:, None] != y[None, :]\n",
    "    D = np.where(mask, D, np.inf)\n",
    "    \n",
    "    # En yakın n_pairs çifti al\n",
    "    flat_idx = np.argsort(D, axis=None)[:n_pairs]\n",
    "    pairs, labels = [], []\n",
    "    for idx in flat_idx:\n",
    "        i, j = np.unravel_index(idx, D.shape)\n",
    "        pairs.append([X[i], X[j]])\n",
    "        labels.append(0)\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "# ============================================\n",
    "#  2. Basitleştirilmiş Siamese Model\n",
    "# ============================================\n",
    "\n",
    "def build_siamese_network(input_dim):\n",
    "    \"\"\"Daha basit ve sağlam bir Siamese ağı\"\"\"\n",
    "    # Embedding network (base)\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(1e-4))(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # L2 normalize edilmiş embedding\n",
    "    embedding = layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=1))(x)\n",
    "    \n",
    "    base = Model(inp, embedding, name=\"embedder\")\n",
    "    \n",
    "    # Siamese yapısı\n",
    "    a = layers.Input(shape=(input_dim,))\n",
    "    b = layers.Input(shape=(input_dim,))\n",
    "    ea = base(a)\n",
    "    eb = base(b)\n",
    "    \n",
    "    # L2 distance\n",
    "    dist = layers.Lambda(lambda z: tf.reduce_sum(tf.square(z[0] - z[1]),\n",
    "                                                  axis=1, keepdims=True))([ea, eb])\n",
    "    \n",
    "    # Sigmoid ile [0,1] aralığına çevir\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(dist)\n",
    "    \n",
    "    model = Model([a, b], out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model, base\n",
    "\n",
    "# ============================================\n",
    "#  3. Tahmin Fonksiyonu\n",
    "# ============================================\n",
    "\n",
    "def predict_patient_embedding(embedder, x_test, X_train, y_train):\n",
    "    \"\"\"Test örneğinin en yakın hastasını bul\"\"\"\n",
    "    z_test = embedder.predict(x_test.reshape(1, -1), verbose=0)\n",
    "    Z_train = embedder.predict(X_train, verbose=0)\n",
    "    \n",
    "    # Her train örneğine mesafe\n",
    "    d = euclidean_distances(z_test, Z_train)[0]\n",
    "    \n",
    "    # Hasta bazında ortalama mesafe\n",
    "    df = pd.DataFrame({\"patient\": y_train, \"dist\": d})\n",
    "    return df.groupby(\"patient\")[\"dist\"].mean().idxmin()\n",
    "\n",
    "# ============================================\n",
    "#  4. GLOBAL MODEL YAKLAŞIMI (ÖNERİLEN)\n",
    "# ============================================\n",
    "\n",
    "def train_global_model(X, y):\n",
    "    \"\"\"Tüm veriyle tek bir model eğit\"\"\"\n",
    "    print(\"Global model eğitiliyor...\")\n",
    "    \n",
    "    # Tüm positive pairs\n",
    "    Xp, yp = make_positive_pairs(X, y)\n",
    "    print(f\"Positive pairs: {len(yp)}\")\n",
    "    \n",
    "    # 3x daha fazla negative pair\n",
    "    Xn, yn = make_negative_pairs_hard(X, y, n_pairs=len(yp) * 3, n_components=100)\n",
    "    print(f\"Negative pairs: {len(yn)}\")\n",
    "    \n",
    "    # Birleştir\n",
    "    X_pairs = np.vstack([Xp, Xn])\n",
    "    y_pairs = np.hstack([yp, yn])\n",
    "    \n",
    "    # Karıştır\n",
    "    idx = np.random.permutation(len(y_pairs))\n",
    "    X_pairs = X_pairs[idx]\n",
    "    y_pairs = y_pairs[idx]\n",
    "    \n",
    "    # Model\n",
    "    siamese, embedder = build_siamese_network(X.shape[1])\n",
    "    \n",
    "    # Eğit\n",
    "    history = siamese.fit(\n",
    "        [X_pairs[:, 0], X_pairs[:, 1]],\n",
    "        y_pairs,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_split=0.15,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor=\"val_loss\",\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return siamese, embedder, history\n",
    "\n",
    "def evaluate_global_model(embedder, df_all, X, y):\n",
    "    \"\"\"Global model ile CV değerlendirmesi\"\"\"\n",
    "    print(\"\\nGlobal model ile CV değerlendirmesi yapılıyor...\\n\")\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(reidentification_cv(df_all), 1):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Tahmin\n",
    "        pred = predict_patient_embedding(embedder, X_test[0], X_train, y_train)\n",
    "        \n",
    "        y_true.append(y_test[0])\n",
    "        y_pred.append(pred)\n",
    "        \n",
    "        if fold % 50 == 0:\n",
    "            print(f\"Fold {fold}/215 tamamlandı...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GLOBAL MODEL SONUÇLARI\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "# ============================================\n",
    "#  5. PER-FOLD MODEL YAKLAŞIMI (ALTERNATIF)\n",
    "# ============================================\n",
    "\n",
    "def evaluate_per_fold_model(df_all, X, y):\n",
    "    \"\"\"Her fold için ayrı model eğit (yavaş ama leakage-free)\"\"\"\n",
    "    print(\"\\nHer fold için ayrı model eğitiliyor...\\n\")\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(reidentification_cv(df_all), 1):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Train pairs\n",
    "        Xp, yp = make_positive_pairs(X_train, y_train)\n",
    "        Xn, yn = make_negative_pairs_hard(X_train, y_train, \n",
    "                                          n_pairs=len(yp) * 2, \n",
    "                                          n_components=100)\n",
    "        \n",
    "        X_pairs = np.vstack([Xp, Xn])\n",
    "        y_pairs = np.hstack([yp, yn])\n",
    "        \n",
    "        # Yeni model\n",
    "        siamese, embedder = build_siamese_network(X_train.shape[1])\n",
    "        \n",
    "        # Eğit (az epoch, çabuk bitsin)\n",
    "        siamese.fit(\n",
    "            [X_pairs[:, 0], X_pairs[:, 1]],\n",
    "            y_pairs,\n",
    "            epochs=30,\n",
    "            batch_size=64,\n",
    "            verbose=0,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"loss\",\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Embedding kalitesi\n",
    "        Z_train = embedder.predict(X_train, verbose=0)\n",
    "        sil_score = silhouette_score(Z_train, y_train)\n",
    "        silhouette_scores.append(sil_score)\n",
    "        \n",
    "        # Tahmin\n",
    "        pred = predict_patient_embedding(embedder, X_test[0], X_train, y_train)\n",
    "        \n",
    "        y_true.append(y_test[0])\n",
    "        y_pred.append(pred)\n",
    "        \n",
    "        if fold % 50 == 0:\n",
    "            print(f\"Fold {fold}/215 - Silhouette: {sil_score:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PER-FOLD MODEL SONUÇLARI\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Ortalama Silhouette Score: {np.mean(silhouette_scores):.3f}\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "# ============================================\n",
    "#  6. ANA ÇALIŞTIRMA\n",
    "# ============================================\n",
    "\n",
    "# Veriyi hazırla (df_all, X, y zaten mevcut olmalı)\n",
    "# X = np.vstack(df_all[\"expression\"].values)\n",
    "# y = df_all[\"patient\"].values\n",
    "\n",
    "# YÖNTEM 1: GLOBAL MODEL (ÖNERİLEN - HIZLI)\n",
    "siamese_global, embedder_global, history = train_global_model(X, y)\n",
    "evaluate_global_model(embedder_global, df_all, X, y)\n",
    "\n",
    "# YÖNTEM 2: PER-FOLD MODEL (İSTEĞE BAĞLI - YAVAŞ)\n",
    "# evaluate_per_fold_model(df_all, X, y)\n",
    "\n",
    "# Embedding kalitesini kontrol et\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EMBEDDING KALİTESİ\")\n",
    "print(\"=\"*50)\n",
    "Z_all = embedder_global.predict(X, verbose=0)\n",
    "sil_all = silhouette_score(Z_all, y)\n",
    "print(f\"Tüm veri Silhouette Score: {sil_all:.3f}\")\n",
    "print(f\"(> 0.5 iyi, > 0.7 çok iyi)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e1bbe-cfb9-41b5-acc3-3dbac2a256e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "\n",
    "# ============================================\n",
    "#  FONKSIYONLAR\n",
    "# ============================================\n",
    "\n",
    "def reidentification_cv(df):\n",
    "    \"\"\"Her hasta için her örneği bir kez test olarak bırak\"\"\"\n",
    "    for patient, df_p in df.groupby(\"patient\"):\n",
    "        for test_idx in df_p.index:\n",
    "            train_idx = df.index.difference([test_idx])\n",
    "            yield train_idx.to_numpy(), np.array([test_idx])\n",
    "\n",
    "def make_positive_pairs(X, y):\n",
    "    \"\"\"Aynı hastadan gelen tüm çiftleri oluştur\"\"\"\n",
    "    pairs, labels = [], []\n",
    "    for p in np.unique(y):\n",
    "        idx = np.where(y == p)[0]\n",
    "        for i in range(len(idx)):\n",
    "            for j in range(i + 1, len(idx)):\n",
    "                pairs.append([X[idx[i]], X[idx[j]]])\n",
    "                labels.append(1)\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "def make_negative_pairs_hard(X, y, n_pairs, n_components=150):\n",
    "    \"\"\"En yakın (hard) negative çiftleri bul\"\"\"\n",
    "    pca = PCA(n_components=min(n_components, X.shape[1]))\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    D = euclidean_distances(X_pca, X_pca)\n",
    "    mask = y[:, None] != y[None, :]\n",
    "    D = np.where(mask, D, np.inf)\n",
    "    \n",
    "    flat_idx = np.argsort(D, axis=None)[:n_pairs]\n",
    "    pairs, labels = [], []\n",
    "    for idx in flat_idx:\n",
    "        i, j = np.unravel_index(idx, D.shape)\n",
    "        pairs.append([X[i], X[j]])\n",
    "        labels.append(0)\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "# ============================================\n",
    "#  MODEL - CONTRASTIVE LOSS\n",
    "# ============================================\n",
    "\n",
    "def build_siamese_contrastive(input_dim):\n",
    "    \"\"\"Contrastive loss kullanan Siamese network - GÜÇLÜ VERSİYON\"\"\"\n",
    "    # Base embedding network\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Daha derin ve geniş\n",
    "    x = layers.Dense(512, activation=\"relu\", \n",
    "                     kernel_regularizer=regularizers.l2(1e-4))(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    embedding = layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=1))(x)\n",
    "    \n",
    "    base = Model(inp, embedding, name=\"embedder\")\n",
    "    \n",
    "    # Siamese structure\n",
    "    a = layers.Input(shape=(input_dim,))\n",
    "    b = layers.Input(shape=(input_dim,))\n",
    "    ea = base(a)\n",
    "    eb = base(b)\n",
    "    \n",
    "    # Euclidean distance\n",
    "    dist = layers.Lambda(\n",
    "        lambda z: tf.sqrt(tf.reduce_sum(tf.square(z[0] - z[1]), axis=1, keepdims=True) + 1e-6)\n",
    "    )([ea, eb])\n",
    "    \n",
    "    model = Model([a, b], dist)\n",
    "    \n",
    "    # Contrastive loss\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        \"\"\"y_true: 1=similar, 0=dissimilar\"\"\"\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        square_pred = tf.square(y_pred)\n",
    "        margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "        return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=contrastive_loss,\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model, base\n",
    "\n",
    "# ============================================\n",
    "#  GLOBAL MODEL EĞİTİMİ (PDF Cell 48 mantığı)\n",
    "# ============================================\n",
    "\n",
    "def train_global_siamese(X, y):\n",
    "    \"\"\"TÜM VERİYLE tek model eğit - PDF Cell 48-52 gibi\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"GLOBAL SIAMESE MODEL EĞİTİLİYOR (TÜM VERİ)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # TÜM positive pairs\n",
    "    Xp, yp = make_positive_pairs(X, y)\n",
    "    print(f\"✓ Positive pairs: {len(yp)}\")\n",
    "    \n",
    "    # 5x negative pairs (daha fazla!)\n",
    "    Xn, yn = make_negative_pairs_hard(X, y, n_pairs=len(yp) * 5, n_components=200)\n",
    "    print(f\"✓ Negative pairs: {len(yn)}\")\n",
    "    \n",
    "    # Birleştir ve karıştır\n",
    "    X_pairs = np.vstack([Xp, Xn])\n",
    "    y_pairs = np.hstack([yp, yn])\n",
    "    \n",
    "    idx = np.random.permutation(len(y_pairs))\n",
    "    X_pairs = X_pairs[idx]\n",
    "    y_pairs = y_pairs[idx]\n",
    "    \n",
    "    # Model oluştur\n",
    "    siamese, embedder = build_siamese_contrastive(X.shape[1])\n",
    "    \n",
    "    # Eğit (UZUN EĞİTİM - PDF'te 40 epoch)\n",
    "    history = siamese.fit(\n",
    "        [X_pairs[:, 0], X_pairs[:, 1]],\n",
    "        y_pairs,\n",
    "        epochs=60,\n",
    "        batch_size=64,\n",
    "        validation_split=0.15,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor=\"val_loss\",\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return siamese, embedder, history\n",
    "\n",
    "# ============================================\n",
    "#  TAHMİN FONKSİYONU\n",
    "# ============================================\n",
    "\n",
    "def predict_patient_embedding(embedder, x_test, X_train, y_train):\n",
    "    \"\"\"Test örneğinin en yakın hastasını bul\"\"\"\n",
    "    z_test = embedder.predict(x_test.reshape(1, -1), verbose=0)\n",
    "    Z_train = embedder.predict(X_train, verbose=0)\n",
    "    \n",
    "    d = euclidean_distances(z_test, Z_train)[0]\n",
    "    df = pd.DataFrame({\"patient\": y_train, \"dist\": d})\n",
    "    return df.groupby(\"patient\")[\"dist\"].mean().idxmin()\n",
    "\n",
    "# ============================================\n",
    "#  DEĞERLENDİRME\n",
    "# ============================================\n",
    "\n",
    "def evaluate_global_siamese(embedder, df_all, X, y):\n",
    "    \"\"\"Global model ile CV değerlendirmesi\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GLOBAL MODEL DEĞERLENDİRMESİ (PDF Cell 52 mantığı)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(reidentification_cv(df_all), 1):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        pred = predict_patient_embedding(embedder, X_test[0], X_train, y_train)\n",
    "        \n",
    "        y_true.append(y_test[0])\n",
    "        y_pred.append(pred)\n",
    "        \n",
    "        if fold % 50 == 0:\n",
    "            print(f\"Fold {fold}/215 tamamlandı...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SONUÇLAR\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# ============================================\n",
    "#  EMBEDDING KALİTESİ\n",
    "# ============================================\n",
    "\n",
    "def analyze_embeddings(embedder, X, y):\n",
    "    \"\"\"Embedding kalitesini analiz et\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EMBEDDING KALİTE ANALİZİ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    Z = embedder.predict(X, verbose=0)\n",
    "    \n",
    "    # Silhouette score\n",
    "    sil = silhouette_score(Z, y)\n",
    "    print(f\"Silhouette Score: {sil:.3f}\")\n",
    "    print(f\"  → < 0: Kötü ayrışma\")\n",
    "    print(f\"  → 0-0.3: Zayıf ayrışma\")\n",
    "    print(f\"  → 0.3-0.5: Orta ayrışma\")\n",
    "    print(f\"  → 0.5-0.7: İyi ayrışma\")\n",
    "    print(f\"  → > 0.7: Çok iyi ayrışma\")\n",
    "    \n",
    "    # Hasta içi vs hasta arası mesafe\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    D = squareform(pdist(Z, 'euclidean'))\n",
    "    \n",
    "    intra_dists = []\n",
    "    inter_dists = []\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        for j in range(i+1, len(y)):\n",
    "            if y[i] == y[j]:\n",
    "                intra_dists.append(D[i, j])\n",
    "            else:\n",
    "                inter_dists.append(D[i, j])\n",
    "    \n",
    "    intra_mean = np.mean(intra_dists)\n",
    "    inter_mean = np.mean(inter_dists)\n",
    "    ratio = inter_mean / intra_mean\n",
    "    \n",
    "    print(f\"\\nMesafe Analizi:\")\n",
    "    print(f\"  Hasta içi mesafe: {intra_mean:.3f} (aynı hasta örnekleri)\")\n",
    "    print(f\"  Hasta arası mesafe: {inter_mean:.3f} (farklı hasta örnekleri)\")\n",
    "    print(f\"  Oran (inter/intra): {ratio:.3f}\")\n",
    "    print(f\"  → Oran > 1.5 ise model iyi ayırt ediyor\")\n",
    "\n",
    "# ============================================\n",
    "#  SEÇ A: HYBRID APPROACH (ÖNERİLEN)\n",
    "# ============================================\n",
    "\n",
    "def hybrid_approach(X, y, df_all):\n",
    "    \"\"\"\n",
    "    Global embedding eğit + Embedding üzerinde LogReg CV\n",
    "    Bu yaklaşım leakage içermez çünkü:\n",
    "    - Embedding sadece representation öğreniyor (classification değil)\n",
    "    - LogReg CV sırasında test fold'u görülmüyor\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"HYBRID APPROACH: Global Embedding + LogReg CV\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ADIM 1: Global embedding eğit\n",
    "    print(\"\\n[1/3] Global Siamese embedding eğitiliyor...\")\n",
    "    siamese_global, embedder_global, _ = train_global_siamese(X, y)\n",
    "    \n",
    "    # ADIM 2: Tüm veriyi embedding space'e dönüştür\n",
    "    print(\"\\n[2/3] Tüm veri embedding space'e dönüştürülüyor...\")\n",
    "    Z = embedder_global.predict(X, verbose=0)\n",
    "    print(f\"✓ Original shape: {X.shape}\")\n",
    "    print(f\"✓ Embedding shape: {Z.shape}\")\n",
    "    \n",
    "    # ADIM 3: Embedding üzerinde CV ile LogReg\n",
    "    print(\"\\n[3/3] Embedding üzerinde CV yapılıyor...\")\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(reidentification_cv(df_all), 1):\n",
    "        Z_train, Z_test = Z[train_idx], Z[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        clf = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            LogisticRegression(max_iter=5000, C=1.0)\n",
    "        )\n",
    "        clf.fit(Z_train, y_train)\n",
    "        pred = clf.predict(Z_test)\n",
    "        \n",
    "        y_true.append(y_test[0])\n",
    "        y_pred.append(pred[0])\n",
    "        \n",
    "        if fold % 50 == 0:\n",
    "            print(f\"  Fold {fold}/215 tamamlandı...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HYBRID APPROACH SONUÇLARI\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    \n",
    "    return embedder_global, y_true, y_pred\n",
    "\n",
    "# ============================================\n",
    "#  ANA ÇALIŞTIRMA\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" YÖNTEM 1: PURE SIAMESE (Leakage var - referans için)\")\n",
    "print(\"=\"*70)\n",
    "print(\"ADIM 1: Global model eğitimi (tüm veriyle)\")\n",
    "siamese_global, embedder_global, history = train_global_siamese(X, y)\n",
    "\n",
    "print(\"\\nADIM 2: Embedding kalitesini kontrol et\")\n",
    "analyze_embeddings(embedder_global, X, y)\n",
    "\n",
    "print(\"\\nADIM 3: Cross-validation ile değerlendir\")\n",
    "y_true_siamese, y_pred_siamese = evaluate_global_siamese(embedder_global, df_all, X, y)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" YÖNTEM 2: HYBRID APPROACH (Leakage yok - asıl sonuç)\")\n",
    "print(\"=\"*70)\n",
    "embedder_hybrid, y_true_hybrid, y_pred_hybrid = hybrid_approach(X, y, df_all)\n",
    "\n",
    "# ============================================\n",
    "#  KARŞILAŞTIRMA\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" SONUÇLARIN KARŞILAŞTIRILMASI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'LogReg + PCA (PDF)',\n",
    "        'SVM + PCA (PDF)', \n",
    "        'Pure Siamese (leakage)',\n",
    "        'Hybrid Siamese + LogReg'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        0.488,\n",
    "        0.409,\n",
    "        accuracy_score(y_true_siamese, y_pred_siamese),\n",
    "        accuracy_score(y_true_hybrid, y_pred_hybrid)\n",
    "    ],\n",
    "    'F1-Score (macro)': [\n",
    "        0.508,\n",
    "        0.401,\n",
    "        f1_score(y_true_siamese, y_pred_siamese, average='macro', zero_division=0),\n",
    "        f1_score(y_true_hybrid, y_pred_hybrid, average='macro', zero_division=0)\n",
    "    ],\n",
    "    'Leakage': ['No', 'No', 'Yes', 'No']\n",
    "})\n",
    "\n",
    "print(\"\\n\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" YORUMLAR\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Pure Siamese yüksek accuracy gösteriyor ama LEAKAGE var\")\n",
    "print(\"2. Hybrid approach leakage-free ve LogReg'ten daha iyi olmalı\")\n",
    "print(\"3. Eğer Hybrid < LogReg ise, embedding kalitesi yetersiz demektir\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a66c3-ae67-4a65-b538-92f8ee9051ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio için tablo\n",
    "results_final = pd.DataFrame({\n",
    "    'Method': ['LogReg + PCA', 'SVM + PCA', 'Siamese + LogReg'],\n",
    "    'Accuracy': [0.488, 0.409, 0.898],\n",
    "    'F1-Score': [0.508, 0.401, 0.878],\n",
    "    'Architecture': ['Linear', 'RBF Kernel', 'Deep Metric Learning'],\n",
    "    'Embedding Dim': ['-', '-', '64D'],\n",
    "    'Training Time': ['1 min', '2 min', '8 min']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0874f8-948e-4c87-a9ed-9bd9b5f6076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE plot\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Z = embedder_hybrid.predict(X)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "Z_2d = tsne.fit_transform(Z)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, p in enumerate(np.unique(y)):\n",
    "    idx = y == p\n",
    "    plt.scatter(Z_2d[idx, 0], Z_2d[idx, 1], \n",
    "                label=f'Patient {p}', alpha=0.7, s=100)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), ncol=2)\n",
    "plt.title('Siamese Embedding Space (t-SNE)', fontsize=16)\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.tight_layout()\n",
    "plt.savefig('siamese_tsne.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65c2dd-705e-4ecf-a09c-878c3a67ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_true_hybrid, y_pred_hybrid)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Hybrid Siamese Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5ec95-c934-4f22-81ad-ef2a53cdb7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hangi hastalar zor?\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_true_hybrid, y_pred_hybrid, \n",
    "                               output_dict=True, zero_division=0)\n",
    "per_class = pd.DataFrame(report).T[:-3]  # Exclude avg rows\n",
    "per_class = per_class.sort_values('f1-score')\n",
    "\n",
    "print(\"En zor 5 hasta:\")\n",
    "print(per_class.head())\n",
    "\n",
    "print(\"\\nEn kolay 5 hasta:\")\n",
    "print(per_class.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678601b-79af-4313-a59c-67be713aff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASLIYORUZ 31 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c6ceb-a291-475d-b461-6ffc3af4514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reidentification_cv(df):\n",
    "    \"\"\"\n",
    "    Each patient's each timepoint is once used as unknown test sample.\n",
    "    All remaining samples form the leaked training set.\n",
    "    \"\"\"\n",
    "    for patient, df_p in df.groupby(\"patient\"):\n",
    "        for test_idx in df_p.index:\n",
    "            train_idx = df.index.difference([test_idx])\n",
    "            yield train_idx.to_numpy(), np.array([test_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b077604-13f4-4432-ae7c-07a3b8cd4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positive_pairs(X, y):\n",
    "    \"\"\"\n",
    "    All pairs from the same patient (positive pairs).\n",
    "    \"\"\"\n",
    "    pairs, labels = [], []\n",
    "\n",
    "    for p in np.unique(y):\n",
    "        idx = np.where(y == p)[0]\n",
    "        for i in range(len(idx)):\n",
    "            for j in range(i + 1, len(idx)):\n",
    "                pairs.append([X[idx[i]], X[idx[j]]])\n",
    "                labels.append(1)\n",
    "\n",
    "    return np.array(pairs), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45967d05-c9ab-4a80-b684-a700cd417dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_negative_pairs_hard(X, y, n_pairs, n_components=100):\n",
    "    \"\"\"\n",
    "    Hard negative mining:\n",
    "    select most similar pairs belonging to different patients.\n",
    "    \"\"\"\n",
    "\n",
    "    pca = PCA(n_components=min(n_components, X.shape[1]))\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    D = euclidean_distances(X_pca, X_pca)\n",
    "\n",
    "    # mask same-patient pairs\n",
    "    mask = y[:, None] != y[None, :]\n",
    "    D = np.where(mask, D, np.inf)\n",
    "\n",
    "    # select closest different-patient pairs\n",
    "    flat_idx = np.argsort(D, axis=None)[:n_pairs]\n",
    "\n",
    "    pairs, labels = [], []\n",
    "    for idx in flat_idx:\n",
    "        i, j = np.unravel_index(idx, D.shape)\n",
    "        pairs.append([X[i], X[j]])\n",
    "        labels.append(0)\n",
    "\n",
    "    return np.array(pairs), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa5908-2a05-491c-82ea-09739e30437a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b5958-61d0-4aa4-9099-9b43e17b77b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90ebc1-fbbb-4ce6-b180-fe41fc45c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_negative_pairs_random(X, y, n_pairs):\n",
    "    \"\"\"\n",
    "    Random negative pairs:\n",
    "    randomly sampled pairs from different patients.\n",
    "    Simulates a weaker attacker.\n",
    "    \"\"\"\n",
    "\n",
    "    pairs, labels = [], []\n",
    "    patients = np.unique(y)\n",
    "\n",
    "    for _ in range(n_pairs):\n",
    "        p1, p2 = np.random.choice(patients, 2, replace=False)\n",
    "        i = np.random.choice(np.where(y == p1)[0])\n",
    "        j = np.random.choice(np.where(y == p2)[0])\n",
    "\n",
    "        pairs.append([X[i], X[j]])\n",
    "        labels.append(0)\n",
    "\n",
    "    return np.array(pairs), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe6a83-8054-4134-afe5-539ea2e1e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_negative_pairs_hard(X, y, n_pairs, n_components=100):\n",
    "    \"\"\"\n",
    "    Hard negative mining:\n",
    "    select most similar pairs belonging to different patients.\n",
    "    \"\"\"\n",
    "\n",
    "    pca = PCA(n_components=min(n_components, X.shape[1]))\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    D = euclidean_distances(X_pca, X_pca)\n",
    "\n",
    "    # mask same-patient pairs\n",
    "    mask = y[:, None] != y[None, :]\n",
    "    D = np.where(mask, D, np.inf)\n",
    "\n",
    "    # select closest different-patient pairs\n",
    "    flat_idx = np.argsort(D, axis=None)[:n_pairs]\n",
    "\n",
    "    pairs, labels = [], []\n",
    "    for idx in flat_idx:\n",
    "        i, j = np.unravel_index(idx, D.shape)\n",
    "        pairs.append([X[i], X[j]])\n",
    "        labels.append(0)\n",
    "\n",
    "    return np.array(pairs), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df96013-85e0-455e-9a1c-de86a8afdbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese_network(input_dim):\n",
    "    \"\"\"\n",
    "    Siamese network for metric learning on miRNA profiles.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Embedding network ----\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    x = layers.Dense(256, activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(1e-4))(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Dense(128, activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    embedding = layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=1))(x)\n",
    "\n",
    "    embedder = Model(inp, embedding, name=\"embedder\")\n",
    "\n",
    "    # ---- Siamese structure ----\n",
    "    a = layers.Input(shape=(input_dim,))\n",
    "    b = layers.Input(shape=(input_dim,))\n",
    "\n",
    "    ea = embedder(a)\n",
    "    eb = embedder(b)\n",
    "\n",
    "    dist = layers.Lambda(\n",
    "        lambda z: tf.reduce_sum(tf.square(z[0] - z[1]), axis=1, keepdims=True)\n",
    "    )([ea, eb])\n",
    "\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(dist)\n",
    "\n",
    "    siamese = Model([a, b], out)\n",
    "    siamese.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=\"binary_crossentropy\"\n",
    "    )\n",
    "\n",
    "    return siamese, embedder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783dfc3-3e39-4ecb-829a-9d85a7db1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_patient_embedding(embedder, x_test, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Identify patient by nearest mean embedding distance.\n",
    "    \"\"\"\n",
    "\n",
    "    z_test = embedder.predict(x_test.reshape(1, -1), verbose=0)\n",
    "    Z_train = embedder.predict(X_train, verbose=0)\n",
    "\n",
    "    dists = euclidean_distances(z_test, Z_train)[0]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"patient\": y_train,\n",
    "        \"dist\": dists\n",
    "    })\n",
    "\n",
    "    return df.groupby(\"patient\")[\"dist\"].mean().idxmin()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3bb98-cdd6-4944-b5a9-e8ec7ce41b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = [], []\n",
    "\n",
    "for train_idx, test_idx in reidentification_cv(df_all):\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # pairs only from leaked training data\n",
    "    Xp, yp = make_positive_pairs(X_train, y_train)\n",
    "    Xn, yn = make_negative_pairs_hard(X_train, y_train, n_pairs=len(yp))\n",
    "\n",
    "    X_pairs = np.vstack([Xp, Xn])\n",
    "    y_pairs = np.hstack([yp, yn])\n",
    "\n",
    "    siamese, embedder = build_siamese_network(X_train.shape[1])\n",
    "\n",
    "    siamese.fit(\n",
    "        [X_pairs[:,0], X_pairs[:,1]],\n",
    "        y_pairs,\n",
    "        epochs=30,\n",
    "        batch_size=64,\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"loss\", patience=5, restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pred = predict_patient_embedding(embedder, X_test[0], X_train, y_train)\n",
    "\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(pred)\n",
    "\n",
    "print(\"\\n=== Leakage-Free Strong Attacker ===\\n\")\n",
    "print(classification_report(y_true, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529bc4f-bc1e-4576-a18a-1522928ab84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train once on ALL data (intentional leakage)\n",
    "Xp, yp = make_positive_pairs(X, y)\n",
    "Xn, yn = make_negative_pairs_hard(X, y, n_pairs=len(yp)*3)\n",
    "\n",
    "X_pairs = np.vstack([Xp, Xn])\n",
    "y_pairs = np.hstack([yp, yn])\n",
    "\n",
    "siamese, embedder = build_siamese_network(X.shape[1])\n",
    "\n",
    "siamese.fit(\n",
    "    [X_pairs[:,0], X_pairs[:,1]],\n",
    "    y_pairs,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.15,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for train_idx, test_idx in reidentification_cv(df_all):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    pred = predict_patient_embedding(embedder, X_test[0], X_train, y_train)\n",
    "\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(pred)\n",
    "\n",
    "print(\"\\n=== Leak-Aware Upper-Bound Attacker ===\\n\")\n",
    "print(classification_report(y_true, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987e8e0-4972-4b67-85c9-9834f0b92a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = [], []\n",
    "\n",
    "for train_idx, test_idx in reidentification_cv(df_all):\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # pairs only from leaked training data\n",
    "    Xp, yp = make_positive_pairs(X_train, y_train)\n",
    "    Xn, yn = make_negative_pairs_random(X_train, y_train, n_pairs=len(yp))\n",
    "\n",
    "    X_pairs = np.vstack([Xp, Xn])\n",
    "    y_pairs = np.hstack([yp, yn])\n",
    "\n",
    "    siamese, embedder = build_siamese_network(X_train.shape[1])\n",
    "\n",
    "    siamese.fit(\n",
    "        [X_pairs[:,0], X_pairs[:,1]],\n",
    "        y_pairs,\n",
    "        epochs=25,\n",
    "        batch_size=64,\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"loss\", patience=5, restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pred = predict_patient_embedding(embedder, X_test[0], X_train, y_train)\n",
    "\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(pred)\n",
    "\n",
    "print(\"\\n=== Leakage-Free WEAK Attacker (Random Negatives) ===\\n\")\n",
    "print(classification_report(y_true, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa72e900-fe98-4437-8193-79a8de996772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (miRNA-venv)",
   "language": "python",
   "name": "mirna-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
